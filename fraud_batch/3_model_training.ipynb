{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "becf52d9",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 03: Model training & UI Exploration</span>\n",
    "\n",
    "<span style=\"font-width:bold; font-size: 1.4rem;\">In this last notebook, we will train a model on the dataset we created in the previous tutorial. We will train our model using standard Python and Scikit-learn, although it could just as well be trained with other machine learning frameworks such as PySpark, TensorFlow, and PyTorch. We will also show some of the exploration that can be done in Hopsworks, notably the search functions and the lineage. </span>\n",
    "\n",
    "## **üóíÔ∏è This notebook is divided in 3 main sections:** \n",
    "1. **Loading the training data**\n",
    "2. **Train the model**\n",
    "3. **Explore feature groups and views** via the UI.\n",
    "\n",
    "![tutorial-flow](images/03_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8fc5f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "\n",
    "conn = hsfs.connection()\n",
    "fs = conn.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7757659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = fs.get_feature_view(\"transactions_view\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35344a1b",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ‚ú® Load Training Data </span>\n",
    "\n",
    "First, we'll need to fetch the training dataset that we created in the previous notebook. We will use January - February data training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "657f0e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: pyarrow.hdfs.HadoopFileSystem is deprecated as of 2.0.0, please use pyarrow.fs.HadoopFileSystem instead.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "train_jan_feb_x, train_jan_feb_y = feature_view.get_training_data(1)\n",
    "train_mar_x, train_mar_y = feature_view.get_training_data(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6ace602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>age_at_transaction</th>\n",
       "      <th>days_until_card_expires</th>\n",
       "      <th>loc_delta</th>\n",
       "      <th>loc_delta_mavg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002478</td>\n",
       "      <td>0.451919</td>\n",
       "      <td>0.873791</td>\n",
       "      <td>0.106689</td>\n",
       "      <td>0.108080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.628477</td>\n",
       "      <td>0.512349</td>\n",
       "      <td>0.045635</td>\n",
       "      <td>0.046230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.191015</td>\n",
       "      <td>0.033169</td>\n",
       "      <td>0.853372</td>\n",
       "      <td>0.236723</td>\n",
       "      <td>0.239810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002814</td>\n",
       "      <td>0.387324</td>\n",
       "      <td>0.307864</td>\n",
       "      <td>0.099704</td>\n",
       "      <td>0.156792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.632300</td>\n",
       "      <td>0.813488</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38978</th>\n",
       "      <td>0.003110</td>\n",
       "      <td>0.665616</td>\n",
       "      <td>0.738367</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38979</th>\n",
       "      <td>0.001577</td>\n",
       "      <td>0.578103</td>\n",
       "      <td>0.563284</td>\n",
       "      <td>0.101315</td>\n",
       "      <td>0.102636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38980</th>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.574697</td>\n",
       "      <td>0.763167</td>\n",
       "      <td>0.467712</td>\n",
       "      <td>0.348457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38981</th>\n",
       "      <td>0.000972</td>\n",
       "      <td>0.614414</td>\n",
       "      <td>0.075533</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38982</th>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.921640</td>\n",
       "      <td>0.689892</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38963 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         amount  age_at_transaction  days_until_card_expires  loc_delta  \\\n",
       "0      0.002478            0.451919                 0.873791   0.106689   \n",
       "1      0.000247            0.628477                 0.512349   0.045635   \n",
       "2      0.191015            0.033169                 0.853372   0.236723   \n",
       "3      0.002814            0.387324                 0.307864   0.099704   \n",
       "4      0.001679            0.632300                 0.813488   0.000029   \n",
       "...         ...                 ...                      ...        ...   \n",
       "38978  0.003110            0.665616                 0.738367   0.000054   \n",
       "38979  0.001577            0.578103                 0.563284   0.101315   \n",
       "38980  0.001344            0.574697                 0.763167   0.467712   \n",
       "38981  0.000972            0.614414                 0.075533   0.000055   \n",
       "38982  0.000717            0.921640                 0.689892   0.000069   \n",
       "\n",
       "       loc_delta_mavg  \n",
       "0            0.108080  \n",
       "1            0.046230  \n",
       "2            0.239810  \n",
       "3            0.156792  \n",
       "4            0.000030  \n",
       "...               ...  \n",
       "38978        0.000055  \n",
       "38979        0.102636  \n",
       "38980        0.348457  \n",
       "38981        0.000055  \n",
       "38982        0.000070  \n",
       "\n",
       "[38963 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_jan_feb_x[train_jan_feb_y.fraud_label==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a8809f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_fraud = train_jan_feb_x[train_jan_feb_y.fraud_label!=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0f2a079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>age_at_transaction</th>\n",
       "      <th>days_until_card_expires</th>\n",
       "      <th>loc_delta</th>\n",
       "      <th>loc_delta_mavg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002478</td>\n",
       "      <td>0.451919</td>\n",
       "      <td>0.873791</td>\n",
       "      <td>0.106689</td>\n",
       "      <td>0.108080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.628477</td>\n",
       "      <td>0.512349</td>\n",
       "      <td>0.045635</td>\n",
       "      <td>0.046230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.191015</td>\n",
       "      <td>0.033169</td>\n",
       "      <td>0.853372</td>\n",
       "      <td>0.236723</td>\n",
       "      <td>0.239810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002814</td>\n",
       "      <td>0.387324</td>\n",
       "      <td>0.307864</td>\n",
       "      <td>0.099704</td>\n",
       "      <td>0.156792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.632300</td>\n",
       "      <td>0.813488</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38978</th>\n",
       "      <td>0.003110</td>\n",
       "      <td>0.665616</td>\n",
       "      <td>0.738367</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38979</th>\n",
       "      <td>0.001577</td>\n",
       "      <td>0.578103</td>\n",
       "      <td>0.563284</td>\n",
       "      <td>0.101315</td>\n",
       "      <td>0.102636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38980</th>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.574697</td>\n",
       "      <td>0.763167</td>\n",
       "      <td>0.467712</td>\n",
       "      <td>0.348457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38981</th>\n",
       "      <td>0.000972</td>\n",
       "      <td>0.614414</td>\n",
       "      <td>0.075533</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38982</th>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.921640</td>\n",
       "      <td>0.689892</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38963 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         amount  age_at_transaction  days_until_card_expires  loc_delta  \\\n",
       "0      0.002478            0.451919                 0.873791   0.106689   \n",
       "1      0.000247            0.628477                 0.512349   0.045635   \n",
       "2      0.191015            0.033169                 0.853372   0.236723   \n",
       "3      0.002814            0.387324                 0.307864   0.099704   \n",
       "4      0.001679            0.632300                 0.813488   0.000029   \n",
       "...         ...                 ...                      ...        ...   \n",
       "38978  0.003110            0.665616                 0.738367   0.000054   \n",
       "38979  0.001577            0.578103                 0.563284   0.101315   \n",
       "38980  0.001344            0.574697                 0.763167   0.467712   \n",
       "38981  0.000972            0.614414                 0.075533   0.000055   \n",
       "38982  0.000717            0.921640                 0.689892   0.000069   \n",
       "\n",
       "       loc_delta_mavg  \n",
       "0            0.108080  \n",
       "1            0.046230  \n",
       "2            0.239810  \n",
       "3            0.156792  \n",
       "4            0.000030  \n",
       "...               ...  \n",
       "38978        0.000055  \n",
       "38979        0.102636  \n",
       "38980        0.348457  \n",
       "38981        0.000055  \n",
       "38982        0.000070  \n",
       "\n",
       "[38963 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6909da48",
   "metadata": {},
   "source": [
    "We will train a model to predict `fraud_label` given the rest of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee49392",
   "metadata": {},
   "source": [
    "Let's check the distribution of our target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f09ca85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fraud_label\n",
       "0              0.999487\n",
       "1              0.000513\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_jan_feb_y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "237674ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fraud_label\n",
       "0              1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mar_y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86453bcc",
   "metadata": {},
   "source": [
    "Notice that the distribution is extremely skewed, which is natural considering that fraudulent transactions make up a tiny part of all transactions. Thus we should somehow address the class imbalance. There are many approaches for this, such as weighting the loss function, over- or undersampling, creating synthetic data, or modifying the decision threshold. In this example, we'll use the simplest method which is to just supply a class weight parameter to our learning algorithm. The class weight will affect how much importance is attached to each class, which in our case means that higher importance will be placed on positive (fraudulent) samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c12d76",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üèÉ Train Model</span>\n",
    "\n",
    "Next we'll train a model. Here, we set the class weight of the positive class to be twice as big as the negative class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "438213dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def windowed_dataset(dataset, window_size, batch_size):\n",
    "    ds = dataset.window(window_size, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda x: x.batch(window_size))\n",
    "    return ds.batch(batch_size,True).prefetch(1)\n",
    "\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices(tf.cast(non_fraud[[\"loc_delta\"]].values, tf.float32))\n",
    "training_dataset = windowed_dataset(training_dataset, window_size=32, batch_size=32)\n",
    "training_dataset\n",
    "\n",
    "class GanEncAnomalyDetector(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        super(GanEncAnomalyDetector, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = [4, 4]         \n",
    "        self.d_steps = 3\n",
    "        self.gp_weight = 10 \n",
    "        \n",
    "        self.encoder = self.make_encoder_model(self.input_dim)\n",
    "        self.generator = self.make_generator(self.input_dim, self.latent_dim)\n",
    "        self.discriminator = self.make_discriminator_model(self.input_dim)\n",
    "\n",
    "        self.mse = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "        self.epoch_e_loss_avg = tf.keras.metrics.Mean(name=\"epoch_e_loss_avg\")\n",
    "        self.epoch_d_loss_avg = tf.keras.metrics.Mean(name=\"epoch_d_loss_avg\")\n",
    "        self.epoch_g_loss_avg = tf.keras.metrics.Mean(name=\"epoch_g_loss_avg\")\n",
    "        self.epoch_a_score_avg = tf.keras.metrics.Mean(name=\"epoch_a_score_avg\")\n",
    "\n",
    "        @property\n",
    "        def metrics(self):\n",
    "            return [\n",
    "                self.epoch_e_loss_avg,\n",
    "                self.epoch_d_loss_avg,\n",
    "                self.epoch_g_loss_avg,\n",
    "                self.epoch_a_score_avg,\n",
    "            ]\n",
    "\n",
    "    # define model architectures\n",
    "    def make_encoder_model(self, input_dim):\n",
    "        inputs = tf.keras.layers.Input(shape=(input_dim[0],input_dim[1]))\n",
    "        x = tf.keras.layers.Conv1D(filters = 16, kernel_size= 1,padding='same', kernel_initializer=\"uniform\")(inputs)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        x = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x)\n",
    "        x = tf.keras.layers.Conv1D(filters = 8, kernel_size= 1,padding='same',  kernel_initializer=\"uniform\")(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        x = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x)    \n",
    "        x = tf.keras.layers.Conv1D(filters = 4, kernel_size= 1,padding='same',  kernel_initializer=\"uniform\")(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        x = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x)    \n",
    "        encoder = tf.keras.Model(inputs=inputs, outputs=x, name=\"encoder_model\")\n",
    "        return encoder\n",
    "\n",
    "    def make_generator(self, input_dim, latent_dim):\n",
    "        latent_inputs = tf.keras.layers.Input(shape=(latent_dim[0],latent_dim[1]))\n",
    "        x = tf.keras.layers.Conv1D(filters = 4, kernel_size= 1,padding='same', kernel_initializer=\"uniform\")(latent_inputs) \n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        x = tf.keras.layers.UpSampling1D(2)(x) \n",
    "        x = tf.keras.layers.Conv1D(filters = 8, kernel_size= 1,padding='same', kernel_initializer=\"uniform\")(x) \n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        x = tf.keras.layers.UpSampling1D(2)(x) \n",
    "        x = tf.keras.layers.Conv1D(filters = 16, kernel_size= 1,padding='same', kernel_initializer=\"uniform\")(x) \n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        x = tf.keras.layers.UpSampling1D(2)(x) \n",
    "        x = tf.keras.layers.Conv1D(filters = input_dim[1], kernel_size= 1,padding='same', kernel_initializer=\"uniform\")(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        generator = tf.keras.Model(inputs=latent_inputs, outputs=x, name=\"generator_model\")        \n",
    "        return generator\n",
    "\n",
    "    def make_discriminator_model(self, input_dim):\n",
    "        inputs = tf.keras.layers.Input(shape=(input_dim[0],input_dim[1]))\n",
    "        x = tf.keras.layers.Conv1D(filters = 128, kernel_size= 1,padding='same', kernel_initializer=\"uniform\")(inputs)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        x = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x)\n",
    "        x = tf.keras.layers.Conv1D(filters = 64, kernel_size= 1,padding='same', kernel_initializer=\"uniform\")(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "\n",
    "        # dense output layer\n",
    "        x = tf.keras.layers.Flatten()(x)    \n",
    "        x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "        x = tf.keras.layers.Dense(128)(x)\n",
    "        x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "        prediction = tf.keras.layers.Dense(1)(x)\n",
    "        discriminator = tf.keras.Model(inputs=inputs, outputs=prediction, name=\"discriminator_model\" )               \n",
    "        return discriminator\n",
    "        \n",
    "    # Training function\n",
    "    @tf.function\n",
    "    def train_step(self, real_data):\n",
    "        if isinstance(real_data, tuple):\n",
    "            real_data = real_data[0]\n",
    "\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(real_data)[0]\n",
    "\n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper:\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 1a. Train the encoder and get the encoder loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        # 3. Calculate the gradient penalty\n",
    "        # 4. Multiply this gradient penalty with a constant weight factor\n",
    "        # 5. Add the gradient penalty to the discriminator loss\n",
    "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "        # Train the discriminator first. The original paper recommends training\n",
    "        # the discriminator for `x` more steps (typically 5) as compared to\n",
    "        # one step of the generator. Here we will train it for 3 extra steps\n",
    "        # as compared to 5 to reduce the training time.\n",
    "        for i in range(self.d_steps):\n",
    "            # Get the latent vector\n",
    "            random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim[0], self.latent_dim[1])), \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake data from the latent vector\n",
    "                fake_data = self.generator(random_latent_vectors, training=True)\n",
    "\n",
    "                #(somewhere here step forward?)\n",
    "                # Get the logits for the fake data\n",
    "                fake_logits = self.discriminator(fake_data, training=True)\n",
    "                # Get the logits for the real data\n",
    "                real_logits = self.discriminator(real_data, training=True)\n",
    "\n",
    "                # Calculate the discriminator loss using the fake and real sample logits\n",
    "                d_cost = self.discriminator_loss(real_sample=real_logits, fake_sample=fake_logits)\n",
    "                # Calculate the gradient penalty\n",
    "                gp = self.gradient_penalty(real_data, fake_data)\n",
    "                # Add the gradient penalty to the original discriminator loss\n",
    "                d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(d_gradient, self.discriminator.trainable_variables)\n",
    "            )\n",
    "\n",
    "        # Train the generator\n",
    "        # Get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim[0], self.latent_dim[1]))\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake data using the generator\n",
    "            generated_data = self.generator(random_latent_vectors, training=True)\n",
    "            # Get the discriminator logits for fake data\n",
    "            gen_sample_logits = self.discriminator(generated_data, training=True)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.generator_loss(gen_sample_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # Train the encoder\n",
    "        with tf.GradientTape() as tape:\n",
    "            generated_data = self.generator(random_latent_vectors, training=True)\n",
    "            # Compress generate fake data from the latent vector\n",
    "            encoded_fake_data = self.encoder(generated_data, training=True)\n",
    "            # Reconstruct encoded generate fake data\n",
    "            generator_reconstructed_encoded_fake_data = self.generator(encoded_fake_data, training=True)\n",
    "            # Encode the latent vector\n",
    "            encoded_random_latent_vectors = self.encoder(tf.random.normal(shape=(batch_size, self.input_dim[0], self.input_dim[1])), \n",
    "                                                         training=True)\n",
    "            # Calculate encoder loss\n",
    "            e_loss = self.encoder_loss(generated_data, generator_reconstructed_encoded_fake_data)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        enc_gradient = tape.gradient(e_loss, self.encoder.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.e_optimizer.apply_gradients(\n",
    "            zip(enc_gradient, self.encoder.trainable_variables)\n",
    "        )\n",
    "\n",
    "        anomaly_score = self.compute_anomaly_score(real_data)\n",
    "\n",
    "        self.epoch_d_loss_avg.update_state(d_loss)\n",
    "        self.epoch_g_loss_avg.update_state(g_loss)\n",
    "        self.epoch_e_loss_avg.update_state(e_loss)\n",
    "        self.epoch_a_score_avg.update_state(anomaly_score[\"anomaly_score\"])\n",
    "\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss, \"e_loss\": e_loss, \"anomaly_score\": anomaly_score[\"anomaly_score\"]}\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, input):\n",
    "        if isinstance(input, tuple):\n",
    "            input = input[0]\n",
    "        \n",
    "        batch_size = tf.shape(input)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim[0], self.latent_dim[1]))\n",
    "        # Generate fake data using the generator\n",
    "        generated_data = self.generator(random_latent_vectors, training=False)\n",
    "        # Get the discriminator logits for fake data\n",
    "        gen_sample_logits = self.discriminator(generated_data, training=False)\n",
    "        # Calculate the generator loss\n",
    "        g_loss = self.generator_loss(gen_sample_logits)\n",
    "\n",
    "        \n",
    "        # Compress generate fake data from the latent vector\n",
    "        encoded_fake_data = self.encoder(generated_data, training=False)\n",
    "        # Reconstruct encoded generate fake data\n",
    "        generator_reconstructed_encoded_fake_data = self.generator(encoded_fake_data, training=False)\n",
    "\n",
    "        # Calculate encoder loss\n",
    "        e_loss = self.encoder_loss(generated_data, generator_reconstructed_encoded_fake_data)\n",
    "        \n",
    "        anomaly_score = self.compute_anomaly_score(input)\n",
    "        return {\n",
    "            \"g_loss\": g_loss,\n",
    "            \"e_loss\": e_loss,\n",
    "            \"anomaly_score\": anomaly_score[\"anomaly_score\"]\n",
    "        }\n",
    "    \n",
    "    # define custom server function\n",
    "    @tf.function\n",
    "    def serve_function(self, input):\n",
    "        return self.compute_anomaly_score(input)\n",
    "\n",
    "    def call(self, input):\n",
    "        if isinstance(input, tuple):\n",
    "            input = input[0]\n",
    "        \n",
    "        encoded = self.encoder(input)\n",
    "        decoded = self.generator(encoded)\n",
    "        anomaly_score = self.compute_anomaly_score(input)\n",
    "        return anomaly_score[\"anomaly_score\"], decoded\n",
    "\n",
    "    def compile(self):\n",
    "        super(GanEncAnomalyDetector, self).compile()     \n",
    "        # Define optimizers\n",
    "        self.e_optimizer = tf.keras.optimizers.SGD(lr=0.00001, clipnorm=0.01)        \n",
    "        self.d_optimizer = tf.keras.optimizers.SGD(lr=0.00001, clipnorm=0.01)\n",
    "        self.g_optimizer = tf.keras.optimizers.SGD(lr=0.00001, clipnorm=0.01)\n",
    "\n",
    "    def gradient_penalty(self, real_data, fake_data):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "        This loss is calculated on an interpolated sample\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated sample\n",
    "        real_data_shape = tf.shape(real_data)\n",
    "        alpha = tf.random.normal(shape=[real_data_shape[0], real_data_shape[1], real_data_shape[2]], mean=0.0, stddev=2.0, dtype=tf.dtypes.float32)\n",
    "        #alpha = tf.random_uniform([self.batch_size, 1], minval=-2, maxval=2, dtype=tf.dtypes.float32)\n",
    "        interpolated = (alpha * real_data) + ((1 - alpha) * fake_data)\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated sample.\n",
    "            pred = self.discriminator(interpolated, training=True)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated sample.\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[-2, -1]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp    \n",
    "        \n",
    "    def encoder_loss(self,generated_fake_data, generator_reconstructed_encoded_fake_data):\n",
    "        generator_reconstracted_data = tf.cast(generator_reconstructed_encoded_fake_data, tf.float32)\n",
    "        loss = self.mse(generated_fake_data, generator_reconstracted_data)\n",
    "        beta_cycle_gen = 10.0\n",
    "        loss = loss * beta_cycle_gen\n",
    "        return loss\n",
    "\n",
    "    # Define the loss functions for the discriminator,\n",
    "    # which should be (fake_loss - real_loss).\n",
    "    # We will add the gradient penalty later to this loss function.\n",
    "    def discriminator_loss(self, real_sample, fake_sample):\n",
    "        real_loss = tf.reduce_mean(real_sample)\n",
    "        fake_loss = tf.reduce_mean(fake_sample)\n",
    "        return fake_loss - real_loss\n",
    "\n",
    "    # Define the loss functions for the generator.\n",
    "    def generator_loss(self, fake_sample):\n",
    "        return -tf.reduce_mean(fake_sample)\n",
    "    \n",
    "    def compute_anomaly_score(self, input):\n",
    "        \"\"\"anomaly score.\n",
    "          See https://arxiv.org/pdf/1905.11034.pdf for more details\n",
    "        \"\"\"\n",
    "        # Encode the real data\n",
    "        encoded_real_data = self.encoder(input, training=False)\n",
    "        # Reconstruct encoded real data\n",
    "        generator_reconstructed_encoded_real_data = self.generator(encoded_real_data, training=False)\n",
    "        # Calculate distance between real and reconstructed data (Here may be step forward?)\n",
    "        gen_rec_loss_predict = self.mse(input,generator_reconstructed_encoded_real_data)\n",
    "\n",
    "        # # Compute anomaly score\n",
    "        # real_to_orig_dist_predict = tf.math.reduce_sum(tf.math.pow(encoded_random_latent - encoded_real_data, 2), axis=[-1])\n",
    "        # anomaly_score = (gen_rec_loss_predict * self.anomaly_alpha) + ((1 - self.anomaly_alpha) * real_to_orig_dist_predict)\n",
    "        anomaly_score = gen_rec_loss_predict\n",
    "        return {'anomaly_score': anomaly_score} \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0de21b",
   "metadata": {},
   "source": [
    "Let's see how well it performs on our validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e3bd208",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GanEncAnomalyDetector([32, 1])\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "023a4bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_model (None, 4, 4)\n",
      "generator_model (None, 32, 1)\n",
      "discriminator_model (None, 1)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e078c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(training_dataset,\n",
    "                    epochs=5,\n",
    "                    verbose=0,\n",
    "                    validation_data=training_dataset,\n",
    "                    validation_steps=1,                    \n",
    "                   )\n",
    "history_dict = history.history\n",
    "history_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12371c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9eed3f",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">  Use the model to score transactions </span>\n",
    "We trained model based on January - February data. Now lets retrieve March data and score whether transactions are fraudulend or not   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2810720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-31 21:10:33,992 INFO: USE `fraud_simplified_featurestore`\n",
      "2022-05-31 21:10:34,773 INFO: WITH right_fg0 AS (SELECT *\n",
      "FROM (SELECT `fg1`.`category` `category`, `fg1`.`amount` `amount`, `fg1`.`age_at_transaction` `age_at_transaction`, `fg1`.`days_until_card_expires` `days_until_card_expires`, `fg1`.`loc_delta` `loc_delta`, `fg1`.`cc_num` `join_pk_cc_num`, `fg1`.`datetime` `join_evt_datetime`, `fg0`.`trans_volume_mstd` `trans_volume_mstd`, `fg0`.`trans_volume_mavg` `trans_volume_mavg`, `fg0`.`trans_freq` `trans_freq`, `fg0`.`loc_delta_mavg` `loc_delta_mavg`, RANK() OVER (PARTITION BY `fg1`.`cc_num`, `fg1`.`datetime` ORDER BY `fg0`.`datetime` DESC) pit_rank_hopsworks\n",
      "FROM `fraud_simplified_featurestore`.`transactions_1` `fg1`\n",
      "INNER JOIN `fraud_simplified_featurestore`.`transactions_4h_aggs_1` `fg0` ON `fg1`.`cc_num` = `fg0`.`cc_num` AND `fg1`.`datetime` >= `fg0`.`datetime`\n",
      "WHERE `fg1`.`datetime` >= 1641168001000 AND `fg1`.`datetime` <= 1648771199000) NA\n",
      "WHERE `pit_rank_hopsworks` = 1) (SELECT `right_fg0`.`category` `category`, `right_fg0`.`amount` `amount`, `right_fg0`.`age_at_transaction` `age_at_transaction`, `right_fg0`.`days_until_card_expires` `days_until_card_expires`, `right_fg0`.`loc_delta` `loc_delta`, `right_fg0`.`trans_volume_mstd` `trans_volume_mstd`, `right_fg0`.`trans_volume_mavg` `trans_volume_mavg`, `right_fg0`.`trans_freq` `trans_freq`, `right_fg0`.`loc_delta_mavg` `loc_delta_mavg`\n",
      "FROM right_fg0)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "# Create training datasets based event time filter\n",
    "start_time = int(float(datetime.strptime(\"2022-01-03 00:00:01\", date_format).timestamp()) * 1000)\n",
    "end_time = int(float(datetime.strptime(\"2022-03-31 23:59:59\", date_format).timestamp()) * 1000)\n",
    "\n",
    "feature_view.init_batch_scoring(1)\n",
    "march_transactions = feature_view.get_batch_data(start_time = start_time,  end_time = end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a7490e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>amount</th>\n",
       "      <th>age_at_transaction</th>\n",
       "      <th>days_until_card_expires</th>\n",
       "      <th>loc_delta</th>\n",
       "      <th>trans_volume_mstd</th>\n",
       "      <th>trans_volume_mavg</th>\n",
       "      <th>trans_freq</th>\n",
       "      <th>loc_delta_mavg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.003120</td>\n",
       "      <td>0.091487</td>\n",
       "      <td>0.117163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003120</td>\n",
       "      <td>0.003120</td>\n",
       "      <td>0.003120</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>0.091506</td>\n",
       "      <td>0.116883</td>\n",
       "      <td>0.122200</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>0.132292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.091513</td>\n",
       "      <td>0.116773</td>\n",
       "      <td>0.120125</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.130046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.091518</td>\n",
       "      <td>0.116696</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.065023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000659</td>\n",
       "      <td>0.091615</td>\n",
       "      <td>0.115229</td>\n",
       "      <td>0.040270</td>\n",
       "      <td>0.000659</td>\n",
       "      <td>0.000659</td>\n",
       "      <td>0.000659</td>\n",
       "      <td>0.043596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102971</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.357286</td>\n",
       "      <td>0.467677</td>\n",
       "      <td>0.228904</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.247809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102972</th>\n",
       "      <td>0</td>\n",
       "      <td>0.002816</td>\n",
       "      <td>0.357321</td>\n",
       "      <td>0.467147</td>\n",
       "      <td>0.166719</td>\n",
       "      <td>0.002816</td>\n",
       "      <td>0.002816</td>\n",
       "      <td>0.002816</td>\n",
       "      <td>0.180488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102973</th>\n",
       "      <td>0</td>\n",
       "      <td>0.002934</td>\n",
       "      <td>0.357325</td>\n",
       "      <td>0.467088</td>\n",
       "      <td>0.166874</td>\n",
       "      <td>0.002875</td>\n",
       "      <td>0.002875</td>\n",
       "      <td>0.002875</td>\n",
       "      <td>0.180572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102974</th>\n",
       "      <td>0</td>\n",
       "      <td>0.010322</td>\n",
       "      <td>0.357392</td>\n",
       "      <td>0.466076</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.010322</td>\n",
       "      <td>0.010322</td>\n",
       "      <td>0.010322</td>\n",
       "      <td>0.001244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102975</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>0.357526</td>\n",
       "      <td>0.464050</td>\n",
       "      <td>0.166690</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>0.180457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102976 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        category    amount  age_at_transaction  days_until_card_expires  \\\n",
       "0              4  0.003120            0.091487                 0.117163   \n",
       "1              2  0.002173            0.091506                 0.116883   \n",
       "2              4  0.000008            0.091513                 0.116773   \n",
       "3              4  0.000047            0.091518                 0.116696   \n",
       "4              4  0.000659            0.091615                 0.115229   \n",
       "...          ...       ...                 ...                      ...   \n",
       "102971         0  0.000736            0.357286                 0.467677   \n",
       "102972         0  0.002816            0.357321                 0.467147   \n",
       "102973         0  0.002934            0.357325                 0.467088   \n",
       "102974         0  0.010322            0.357392                 0.466076   \n",
       "102975         0  0.000592            0.357526                 0.464050   \n",
       "\n",
       "        loc_delta  trans_volume_mstd  trans_volume_mavg  trans_freq  \\\n",
       "0        0.000000           0.003120           0.003120    0.003120   \n",
       "1        0.122200           0.002173           0.002173    0.002173   \n",
       "2        0.120125           0.000008           0.000008    0.000008   \n",
       "3        0.000000           0.000028           0.000028    0.000028   \n",
       "4        0.040270           0.000659           0.000659    0.000659   \n",
       "...           ...                ...                ...         ...   \n",
       "102971   0.228904           0.000736           0.000736    0.000736   \n",
       "102972   0.166719           0.002816           0.002816    0.002816   \n",
       "102973   0.166874           0.002875           0.002875    0.002875   \n",
       "102974   0.001149           0.010322           0.010322    0.010322   \n",
       "102975   0.166690           0.000592           0.000592    0.000592   \n",
       "\n",
       "        loc_delta_mavg  \n",
       "0             0.000000  \n",
       "1             0.132292  \n",
       "2             0.130046  \n",
       "3             0.065023  \n",
       "4             0.043596  \n",
       "...                ...  \n",
       "102971        0.247809  \n",
       "102972        0.180488  \n",
       "102973        0.180572  \n",
       "102974        0.001244  \n",
       "102975        0.180457  \n",
       "\n",
       "[102976 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "march_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "506989ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(march_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d588e5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3072841f",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üëì  Exploration</span>\n",
    "In the Hopsworks feature store, the metadata allows for multiple levels of explorations and review. Here we will show a few of those capacities. \n",
    "\n",
    "### üîé <b>Search</b> \n",
    "Using the search function in the ui, you can query any aspect of the feature groups and training data that was previously created. In the gif below we show how the tag we added in the first section can be searched to get all the feature groups with the `PII` tag value.\n",
    "\n",
    "### üìä <b>Statistics</b> \n",
    "We can also enable statistics in one or all the feature groups here we commented the command so that it wouldnt run for too long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3866ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trans_fg = fs.get_feature_group()\n",
    "#trans_fg.update_statistics_config(\n",
    "#       enabled=True, \n",
    "#       correlations=False, \n",
    "#       histograms=False, \n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a148576",
   "metadata": {},
   "source": [
    "\n",
    "### ‚õìÔ∏è <b> Lineage </b> \n",
    "In all the feature groups and feature view you can look at the relation between each abstractions; what feature group created which training dataset and that is used in which model.\n",
    "This allows for a clear undestanding of the pipeline in relation to each element. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e4ba55",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üéÅ  Wrapping things up </span>\n",
    "\n",
    "We have now performed a simple training with training data that we have created in the feature store. This concludes the fisrt module and introduction to the core aspect of the feauture store. In the second module we will introduce streaming and external feature groups for a similar fraud use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}